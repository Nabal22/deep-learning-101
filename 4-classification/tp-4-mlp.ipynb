{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP: Building an MLP from Scratch for CIFAR-10 Classification\n",
    "\n",
    "_Adapted from [Dataflowr Module 5](https://dataflowr.github.io/website/modules/5-stacking-layers/#practicals) by Marc Lelarge_ \n",
    "\n",
    "In this practical, you will implement a Multi-Layer Perceptron (MLP) from scratch using PyTorch to classify images from the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Setup\n",
    "\n",
    "Import the required packages and check PyTorch version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Building the Dataset\n",
    "\n",
    "### About CIFAR-10\n",
    "\n",
    "CIFAR-10 is a dataset of 32x32 color images in 10 classes:\n",
    "- **Classes**: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck\n",
    "- **Training images**: 50,000 (5,000 per class)\n",
    "- **Test images**: 10,000 (1,000 per class)\n",
    "- **Image size**: 32x32 pixels, RGB (3 channels)\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "For an MLP, we need to:\n",
    "1. Convert images to tensors\n",
    "2. Normalize pixel values (mean=0.5, std=0.5 for each channel)\n",
    "3. Flatten 32x32x3 images into 1D vectors of size 3072\n",
    "\n",
    "**TODO 1:** Create a transform pipeline using `transforms.Compose` that:\n",
    "- Converts images to tensors with `transforms.ToTensor()`\n",
    "- Normalizes with `transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading CIFAR-10 Dataset\n",
    "\n",
    "**TODO 2:** Load the CIFAR-10 dataset using `datasets.CIFAR10`.\n",
    "\n",
    "Hints:\n",
    "- Set `root='./data'` to specify download directory\n",
    "- Set `train=True` for training set, `train=False` for test set\n",
    "- Set `download=True` to download the dataset\n",
    "- Use the transform pipeline you created above\n",
    "\n",
    "Check the documentation: [datasets.CIFAR10](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Dataset Properties\n",
    "\n",
    "**TODO 3:** Print the sizes of training and test datasets, and examine the class names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataLoaders\n",
    "\n",
    "DataLoaders handle batching, shuffling, and parallel loading of data.\n",
    "\n",
    "**TODO 4:** Create DataLoaders for training and testing.\n",
    "\n",
    "Hints:\n",
    "- Use `batch_size=128` for training\n",
    "- Use `batch_size=100` for testing\n",
    "- Set `shuffle=True` for training data\n",
    "- Set `shuffle=False` for test data\n",
    "- Set `num_workers=2` for parallel data loading\n",
    "\n",
    "Check the documentation: [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Sample Images\n",
    "\n",
    "**TODO 5:** Visualize a batch of training images using matplotlib.\n",
    "\n",
    "Hints:\n",
    "- Get a batch using `next(iter(train_loader))`\n",
    "- Use `torchvision.utils.make_grid()` to create a grid of images\n",
    "- Don't forget to denormalize before displaying (reverse the normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Understanding MLP Architecture\n",
    "\n",
    "### What is an MLP?\n",
    "\n",
    "A Multi-Layer Perceptron (MLP) is a fully connected neural network consisting of:\n",
    "- **Input layer**: Takes flattened input (3072 values for 32x32x3 images)\n",
    "- **Hidden layers**: One or more layers that learn features\n",
    "- **Output layer**: Produces class probabilities (10 values for CIFAR-10)\n",
    "\n",
    "### Architecture Design\n",
    "\n",
    "We will build an MLP with the following structure:\n",
    "\n",
    "```\n",
    "Input: 32x32x3 image → Flatten → 3072 values\n",
    "   ↓\n",
    "Linear(3072 → 1000)  [Weights: 3072×1000 + 1000 bias = 3,073,000 parameters]\n",
    "   ↓\n",
    "ReLU activation\n",
    "   ↓\n",
    "Linear(1000 → 10)    [Weights: 1000×10 + 10 bias = 10,010 parameters]\n",
    "   ↓\n",
    "Output: 10 class scores\n",
    "```\n",
    "\n",
    "**Total parameters**: ~3.08 million\n",
    "\n",
    "### Understanding Dimensions\n",
    "\n",
    "Each linear layer performs: `output = input @ weights.T + bias`\n",
    "\n",
    "For a batch of size B:\n",
    "- Input shape: `[B, 3072]`\n",
    "- After first linear: `[B, 1000]`\n",
    "- After second linear: `[B, 10]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Linear Activation Functions\n",
    "\n",
    "**Why do we need activation functions?**\n",
    "\n",
    "Without non-linearity, stacking multiple linear layers is equivalent to a single linear layer:\n",
    "```\n",
    "Layer1(Layer2(x)) = W1(W2*x + b2) + b1 = (W1*W2)*x + (W1*b2 + b1) = W3*x + b3\n",
    "```\n",
    "\n",
    "**ReLU (Rectified Linear Unit)**: `ReLU(x) = max(0, x)`\n",
    "- Introduces non-linearity\n",
    "- Computationally efficient\n",
    "- Helps avoid vanishing gradient problem\n",
    "- Most commonly used activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Building the MLP with PyTorch\n",
    "\n",
    "### Using nn.Sequential\n",
    "\n",
    "PyTorch provides `nn.Sequential` to stack layers sequentially. This is convenient for simple architectures.\n",
    "\n",
    "**TODO 6:** Build the MLP model using `nn.Sequential` with:\n",
    "1. `nn.Flatten()` - converts [B, 3, 32, 32] to [B, 3072]\n",
    "2. `nn.Linear(3072, 1000)` - hidden layer\n",
    "3. `nn.ReLU()` - activation function\n",
    "4. `nn.Linear(1000, 10)` - output layer\n",
    "\n",
    "Check the documentation:\n",
    "- [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)\n",
    "- [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
    "- [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Model Architecture\n",
    "\n",
    "**TODO 7:** Print the model architecture and count the total number of parameters.\n",
    "\n",
    "Hints:\n",
    "- Print the model to see its structure\n",
    "- Use `sum(p.numel() for p in model.parameters())` to count parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Forward Pass\n",
    "\n",
    "**TODO 8:** Test the model with a random batch to verify dimensions.\n",
    "\n",
    "Hints:\n",
    "- Create a random tensor of shape `[4, 3, 32, 32]` (batch of 4 images)\n",
    "- Pass it through the model\n",
    "- Verify output shape is `[4, 10]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Training the MLP\n",
    "\n",
    "### Loss Function and Optimizer\n",
    "\n",
    "**Loss Function**: We use Cross-Entropy Loss for multi-class classification\n",
    "- Combines `LogSoftmax` and `NLLLoss`\n",
    "- Measures the difference between predicted probabilities and true labels\n",
    "\n",
    "**Optimizer**: We use Stochastic Gradient Descent (SGD)\n",
    "- Updates weights in the direction that reduces loss\n",
    "- Momentum helps accelerate convergence\n",
    "\n",
    "**TODO 9:** Create the loss function and optimizer.\n",
    "\n",
    "Hints:\n",
    "- Use `nn.CrossEntropyLoss()`\n",
    "- Use `torch.optim.SGD` with:\n",
    "  - `lr=0.01` (learning rate)\n",
    "  - `momentum=0.9`\n",
    "  - `model.parameters()` to optimize all parameters\n",
    "\n",
    "Check the documentation:\n",
    "- [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "- [torch.optim.SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "The training loop consists of:\n",
    "1. **Forward pass**: Compute predictions\n",
    "2. **Loss computation**: Calculate error\n",
    "3. **Backward pass**: Compute gradients using `loss.backward()`\n",
    "4. **Optimizer step**: Update weights using `optimizer.step()`\n",
    "5. **Zero gradients**: Clear gradients with `optimizer.zero_grad()`\n",
    "\n",
    "**TODO 10:** Implement the training function.\n",
    "\n",
    "Hints:\n",
    "- Set model to training mode: `model.train()`\n",
    "- Loop over batches from the dataloader\n",
    "- Move data to device (GPU/CPU)\n",
    "- Perform forward pass, compute loss, backward pass, optimizer step\n",
    "- Track running loss and accuracy\n",
    "- Use `torch.max(outputs, 1)` to get predicted classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "\n",
    "**TODO 11:** Train the model for 10 epochs.\n",
    "\n",
    "Note: Training may take several minutes depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Model Evaluation\n",
    "\n",
    "### Test Function\n",
    "\n",
    "**TODO 12:** Implement a function to evaluate the model on the test set.\n",
    "\n",
    "Hints:\n",
    "- Set model to evaluation mode: `model.eval()`\n",
    "- Use `torch.no_grad()` to disable gradient computation\n",
    "- Calculate test loss and accuracy\n",
    "- Store predictions and probabilities for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on Test Set\n",
    "\n",
    "**TODO 13:** Run the test function and report accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next? \n",
    "\n",
    "What happens if you\n",
    "- switch the training to a GPU? Is it faster?\n",
    "- Remove the `ReLU()`? \n",
    "- Increase the learning rate?\n",
    "- Stack more layers? \n",
    "- Perform more epochs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
