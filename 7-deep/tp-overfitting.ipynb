{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP: Techniques to Avoid Overfitting\n",
    "\n",
    "In this practical, we explore three fundamental techniques to mitigate overfitting:\n",
    "\n",
    "- **Early Stopping** — stop training before the model memorizes the training data\n",
    "- **Batch Normalization** — normalize intermediate activations to stabilize training\n",
    "- **Regularization** — constrain the model via L1/L2 penalties and Dropout\n",
    "\n",
    "We use Fashion-MNIST with a small training subset and a simple MLP as our baseline, then progressively add each technique and compare results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = (\n",
    "    \"cuda:0\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Fashion-MNIST\n",
    "\n",
    "We deliberately use a **small subset** of the training data (5,000 out of 60,000 images) to make overfitting clearly visible.\n",
    "\n",
    "We split this subset into a **training set** and a **validation set** in a 2-to-1 proportion. The validation set will be used for early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.2860,), (0.3530,)),\n",
    "])\n",
    "\n",
    "full_train_set = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_set = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Use a small subset to encourage overfitting\n",
    "subset_size = 5000\n",
    "small_set = torch.utils.data.Subset(full_train_set, range(subset_size))\n",
    "\n",
    "# Split into training (2/3) and validation (1/3)\n",
    "n_train = int(subset_size * 2 / 3)\n",
    "n_val = subset_size - n_train\n",
    "train_set, val_set = torch.utils.data.random_split(small_set, [n_train, n_val])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=256, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=256, shuffle=False)\n",
    "\n",
    "print(f'Training samples: {len(train_set)}')\n",
    "print(f'Validation samples: {len(val_set)}')\n",
    "print(f'Test samples: {len(test_set)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fashion-MNIST (Xiao et al., 2017) is a drop-in replacement for MNIST with 10 clothing categories: T-shirt, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, and Ankle boot. Each image is 28×28 grayscale. It is harder than MNIST, making overfitting easier to observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img, label = full_train_set[i]\n",
    "    ax.imshow(img.squeeze(), cmap='gray')\n",
    "    ax.set_title(class_names[label])\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "The following helpers are provided. No modification is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    \"\"\"Compute loss and accuracy on a dataset.\"\"\"\n",
    "    model.eval()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            running_corrects += (preds == labels).sum().item()\n",
    "            total += inputs.size(0)\n",
    "    return running_loss / total, running_corrects / total\n",
    "\n",
    "\n",
    "def plot_history(histories, title=''):\n",
    "    \"\"\"Plot train/test loss and accuracy for one or more runs.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    for name, h in histories.items():\n",
    "        ax1.plot(h['train_loss'], label=f'{name} (train)')\n",
    "        ax1.plot(h['test_loss'], '--', label=f'{name} (test)')\n",
    "    ax1.set_xlabel('Epoch'); ax1.set_ylabel('Loss'); ax1.set_title('Loss'); ax1.legend()\n",
    "    for name, h in histories.items():\n",
    "        ax2.plot(h['train_acc'], label=f'{name} (train)')\n",
    "        ax2.plot(h['test_acc'], '--', label=f'{name} (test)')\n",
    "    ax2.set_xlabel('Epoch'); ax2.set_ylabel('Accuracy'); ax2.set_title('Accuracy'); ax2.legend()\n",
    "    if title:\n",
    "        fig.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: A Simple MLP that Overfits\n",
    "\n",
    "**TODO:** Implement a simple MLP for classification over 10 classes with the following architecture:\n",
    "\n",
    "- Flatten\n",
    "- Linear(?, 512)\n",
    "- ReLU\n",
    "- Linear(512, 256)\n",
    "- ReLU\n",
    "- Linear(?, ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # TODO\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Implement the Training Loop\n",
    "\n",
    "**Hints:**\n",
    "- Set model to training mode with `model.train()`\n",
    "- For each batch: zero gradients, forward pass, compute loss, backward pass, optimizer step\n",
    "- Use `outputs.argmax(dim=1)` to get predictions\n",
    "- At the end of each epoch, call `evaluate(model, test_loader)` to get test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, optimizer, n_epochs):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    history = {'train_loss': [], 'test_loss': [], 'train_acc': [], 'test_acc': []}\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # TODO: Implement the training step\n",
    "\n",
    "            # TODO: Track statistics\n",
    "            # running_loss += ...\n",
    "            # running_corrects += ...\n",
    "            total += inputs.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = running_corrects / total\n",
    "        test_loss, test_acc = evaluate(model, test_loader)\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_acc'].append(test_acc)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f'Epoch {epoch+1:3d}/{n_epochs} — '\n",
    "                  f'Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} — '\n",
    "                  f'Test Loss: {test_loss:.4f} Acc: {test_acc:.4f}')\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 60\n",
    "\n",
    "torch.manual_seed(42)\n",
    "baseline_model = BaselineMLP().to(device)\n",
    "optimizer = torch.optim.Adam(baseline_model.parameters(), lr=1e-3)\n",
    "baseline_history = train(baseline_model, train_loader, test_loader, optimizer, N_EPOCHS)\n",
    "\n",
    "plot_history({'Baseline': baseline_history})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loss keeps decreasing while the test loss starts increasing after a few epochs. The gap between train and test accuracy is the *generalization gap* — this is overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "\n",
    "### Idea\n",
    "\n",
    "Training a neural network too long lets it memorize the training data. **Early stopping** monitors the validation error and stops training when it starts increasing.\n",
    "\n",
    "The procedure is:\n",
    "1. Split the training data into a training set and a validation set (2-to-1 proportion)\n",
    "2. Train only on the training set and evaluate the per-example error on the validation set once in a while, e.g. **every 5 epochs**\n",
    "3. **Stop** training as soon as the validation error is higher than it was the last time it was checked\n",
    "4. Use the weights the network had at that previous step as the result of the training run\n",
    "\n",
    "**TODO:** Reimplement the training loop with early stopping.\n",
    "\n",
    "1. Every 5 epochs, evaluate on `val_loader` using the `evaluate` function\n",
    "2. If `val_loss` is higher than the previous check: restore the saved weights with `model.load_state_dict(...)` and `break`\n",
    "3. Otherwise: save the current weights with `copy.deepcopy(model.state_dict())`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_early_stopping(model, train_loader, val_loader, test_loader, optimizer, n_epochs, check_every=5):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    history = {'train_loss': [], 'test_loss': [], 'train_acc': [], 'test_acc': []}\n",
    "\n",
    "    last_val_loss = float('inf')\n",
    "    best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # TODO: Implement the training step\n",
    "\n",
    "            # TODO: Track statistics\n",
    "            # running_loss += ...\n",
    "            # running_corrects += ...\n",
    "            total += inputs.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = running_corrects / total\n",
    "        test_loss, test_acc = evaluate(model, test_loader)\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_acc'].append(test_acc)\n",
    "\n",
    "        # TODO: Every check_every epochs, evaluate on val_loader\n",
    "        # - If val_loss > last_val_loss: restore best_weights and break\n",
    "        # - Otherwise: update last_val_loss and save best_weights\n",
    "\n",
    "        if (epoch + 1) % check_every == 0 or epoch == 0:\n",
    "            print(f'Epoch {epoch+1:3d}/{n_epochs} — '\n",
    "                  f'Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} — '\n",
    "                  f'Test Loss: {test_loss:.4f} Acc: {test_acc:.4f}')\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "es_model = BaselineMLP().to(device)\n",
    "optimizer = torch.optim.Adam(es_model.parameters(), lr=1e-3)\n",
    "es_history = train_early_stopping(es_model, train_loader, val_loader, test_loader, optimizer, N_EPOCHS)\n",
    "\n",
    "plot_history({'Baseline': baseline_history, 'Early Stopping': es_history})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the test accuracy of the early-stopped model compare to the baseline at its last epoch? What happens if you change `check_every`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "A common first step for data analysis: normalize the input data, for example to have mean $0$ and standard deviation $1$: `X = (X - X.mean()) / X.std()`\n",
    "\n",
    "Batch normalization applies the same transform to the outputs of a layer by introducing two new learnable parameters: one for the mean and one for the standard deviation.\n",
    "\n",
    "The noise introduced by computing statistics over mini-batches acts as a mild form of regularization.\n",
    "\n",
    "**TODO:** Adapt the baseline network to add a batch normalization after each linear layer\n",
    "\n",
    "- Flatten\n",
    "- Linear\n",
    "- BatchNorm1d\n",
    "- ReLU\n",
    "- Linear\n",
    "- BatchNorm1d\n",
    "- ReLU\n",
    "- Linear\n",
    "\n",
    "**Documentation:** [nn.BatchNorm1d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # TODO: add batch norm layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: apply Linear -> BatchNorm -> ReLU\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "bn_model = BatchNormMLP().to(device)\n",
    "optimizer = torch.optim.Adam(bn_model.parameters(), lr=1e-3)\n",
    "bn_history = train(bn_model, train_loader, test_loader, optimizer, N_EPOCHS)\n",
    "\n",
    "plot_history({'Baseline': baseline_history, 'BatchNorm': bn_history})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does batch normalization reduce the generalization gap? Does the model train faster (reach a given accuracy in fewer epochs)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "### L1 Regularization\n",
    "\n",
    "L1 regularization adds a penalty proportional to the absolute value of the weights:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{reg}} = \\mathcal{L} + \\lambda \\sum_i |w_i|$$\n",
    "\n",
    "L1 tends to produce **sparse** weights (many weights driven exactly to zero), effectively performing feature selection.\n",
    "\n",
    "**TODO:** Reimplement the training loop with L1 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_l1(model, train_loader, test_loader, optimizer, n_epochs, l1_lambda=1e-4):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    history = {'train_loss': [], 'test_loss': [], 'train_acc': [], 'test_acc': []}\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # TODO: Implement the training step\n",
    "\n",
    "            # TODO: Track statistics\n",
    "            # running_loss += ...\n",
    "            # running_corrects += ...\n",
    "            total += inputs.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = running_corrects / total\n",
    "        test_loss, test_acc = evaluate(model, test_loader)\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_acc'].append(test_acc)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f'Epoch {epoch+1:3d}/{n_epochs} — '\n",
    "                  f'Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} — '\n",
    "                  f'Test Loss: {test_loss:.4f} Acc: {test_acc:.4f}')\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "l1_model = BaselineMLP().to(device)\n",
    "optimizer = torch.optim.Adam(l1_model.parameters(), lr=1e-3)\n",
    "l1_history = train_l1(l1_model, train_loader, test_loader, optimizer, N_EPOCHS)\n",
    "\n",
    "plot_history({'Baseline': baseline_history, 'L1': l1_history})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize how L1 regularization promotes sparsity by comparing the weight distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "baseline_weights = baseline_model.fc1.weight.detach().cpu().flatten().numpy()\n",
    "l1_weights = l1_model.fc1.weight.detach().cpu().flatten().numpy()\n",
    "\n",
    "ax1.hist(baseline_weights, bins=50, alpha=0.7)\n",
    "ax1.set_title('Baseline — fc1 weights')\n",
    "ax1.set_xlabel('Weight value')\n",
    "\n",
    "ax2.hist(l1_weights, bins=50, alpha=0.7, color='orange')\n",
    "ax2.set_title('L1 — fc1 weights')\n",
    "ax2.set_xlabel('Weight value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Baseline — weights near zero (<0.01): {(np.abs(baseline_weights) < 0.01).mean():.1%}')\n",
    "print(f'L1       — weights near zero (<0.01): {(np.abs(l1_weights) < 0.01).mean():.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Regularization (Weight Decay)\n",
    "\n",
    "L2 regularization adds a penalty proportional to the squared magnitude of the weights:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{reg}} = \\mathcal{L} + \\lambda \\sum_i w_i^2$$\n",
    "\n",
    "This discourages large weights, pushing the model toward simpler solutions.\n",
    "\n",
    "We can implement it manually, just like L1.\n",
    "\n",
    "**TODO:** Write a `train_l2` function, similar to `train_l1`, that adds an L2 penalty to the loss.\n",
    "\n",
    "**Hints:**\n",
    "- Compute the L2 norm: `l2_norm = sum(p.pow(2).sum() for p in model.parameters())`\n",
    "- Add it to the loss: `loss = loss + l2_lambda * l2_norm`\n",
    "- Try `l2_lambda` values like `1e-3`, `1e-4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_l2(model, train_loader, test_loader, optimizer, n_epochs, l2_lambda=1e-3):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    history = {'train_loss': [], 'test_loss': [], 'train_acc': [], 'test_acc': []}\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # TODO: Implement the training step\n",
    "\n",
    "            # TODO: Track statistics\n",
    "            # running_loss += ...\n",
    "            # running_corrects += ...\n",
    "            total += inputs.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = running_corrects / total\n",
    "        test_loss, test_acc = evaluate(model, test_loader)\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_acc'].append(test_acc)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f'Epoch {epoch+1:3d}/{n_epochs} — '\n",
    "                  f'Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} — '\n",
    "                  f'Test Loss: {test_loss:.4f} Acc: {test_acc:.4f}')\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "l2_model = BaselineMLP().to(device)\n",
    "optimizer = torch.optim.Adam(l2_model.parameters(), lr=1e-3)\n",
    "l2_history = train_l2(l2_model, train_loader, test_loader, optimizer, N_EPOCHS)\n",
    "\n",
    "plot_history({'Baseline': baseline_history, 'L2': l2_history})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2 with `weight_decay`\n",
    "\n",
    "Since L2 regularization is so common, PyTorch provides a shortcut: the `weight_decay` parameter in the optimizer does exactly the same thing.\n",
    "\n",
    "Verify that you get the same result by training the baseline model with `weight_decay=1e-3` using the standard `train` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "l2_wd_model = BaselineMLP().to(device)\n",
    "\n",
    "# TODO: create optimizer with weight_decay=1e-3\n",
    "optimizer = torch.optim.Adam(l2_wd_model.parameters(), lr=1e-3)\n",
    "\n",
    "l2_wd_history = train(l2_wd_model, train_loader, test_loader, optimizer, N_EPOCHS)\n",
    "\n",
    "plot_history({'Baseline': baseline_history, 'L2 (manual)': l2_history, 'L2 (weight_decay)': l2_wd_history})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
