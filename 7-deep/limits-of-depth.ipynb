{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Limits of Depth\n",
    "\n",
    "_Adapted from [Dataflowr Module 14b](https://dataflowr.github.io/website/modules/14b-depth/) by Andrei Bursuc.__\n",
    "\n",
    "In practice, training very deep networks is **hard**. In this notebook, we explore the main obstacles and their solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Use GPU if available\n",
    "device = (\n",
    "    \"cuda:0\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_spiral_data(n=500):\n",
    "    \"\"\"Two interleaved spirals.\"\"\"\n",
    "    t = torch.linspace(0, 4 * np.pi, n // 2)\n",
    "    r = t / (4 * np.pi) * 2\n",
    "    noise = torch.randn(n // 2) * 0.08\n",
    "    X0 = torch.stack([(r + noise) * torch.cos(t), (r + noise) * torch.sin(t)], dim=1)\n",
    "    X1 = torch.stack([(r + noise) * torch.cos(t + np.pi), (r + noise) * torch.sin(t + np.pi)], dim=1)\n",
    "    X = torch.cat([X0, X1], dim=0)\n",
    "    y = torch.cat([torch.zeros(n // 2), torch.ones(n // 2)]).long()\n",
    "    return X, y\n",
    "\n",
    "def plot_decision_boundary(model, X, y, ax=None, title=None, resolution=200):\n",
    "    \"\"\"Plot the decision boundary of a 2D classifier.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    margin = 0.5\n",
    "    x_min, x_max = X[:, 0].min() - margin, X[:, 0].max() + margin\n",
    "    y_min, y_max = X[:, 1].min() - margin, X[:, 1].max() + margin\n",
    "    xx, yy = torch.meshgrid(\n",
    "        torch.linspace(x_min, x_max, resolution),\n",
    "        torch.linspace(y_min, y_max, resolution),\n",
    "        indexing='xy'\n",
    "    )\n",
    "    grid = torch.stack([xx.flatten(), yy.flatten()], dim=1)\n",
    "    with torch.no_grad():\n",
    "        logits = model(grid)\n",
    "        preds = logits.argmax(dim=1).reshape(xx.shape)\n",
    "    cmap_bg = ListedColormap(['#AACCFF', '#FFAAAA'])\n",
    "    ax.contourf(xx.numpy(), yy.numpy(), preds.numpy(), alpha=0.3, cmap=cmap_bg)\n",
    "    ax.scatter(X[y == 0, 0], X[y == 0, 1], c='royalblue', alpha=0.5, s=10, edgecolors='k', linewidths=0.3)\n",
    "    ax.scatter(X[y == 1, 0], X[y == 1, 1], c='crimson', alpha=0.5, s=10, edgecolors='k', linewidths=0.3)\n",
    "    if title:\n",
    "        ax.set_title(title, fontsize=12)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.2)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Vanishing Gradient Problem\n",
    "\n",
    "### The Chain Rule in Deep Networks\n",
    "\n",
    "During backpropagation, the gradient of the loss with respect to the weights of layer $\\ell$ involves a **product of Jacobians** across all subsequent layers:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_\\ell} = \\frac{\\partial \\mathcal{L}}{\\partial f_L} \\cdot \\prod_{k=\\ell+1}^{L} \\frac{\\partial f_k}{\\partial f_{k-1}} \\cdot \\frac{\\partial f_\\ell}{\\partial W_\\ell}\n",
    "$$\n",
    "\n",
    "Each factor $\\frac{\\partial f_k}{\\partial f_{k-1}} = \\text{diag}(\\sigma'(z_k)) \\cdot W_k$ depends on the **derivative of the activation function**.\n",
    "\n",
    "### Sigmoid\n",
    "\n",
    "The sigmoid activation function is:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}, \\qquad \\sigma'(x) = \\sigma(x)(1 - \\sigma(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sigmoid and its derivative\n",
    "x = torch.linspace(-6, 6, 300)\n",
    "sigmoid = torch.sigmoid(x)\n",
    "sigmoid_grad = sigmoid * (1 - sigmoid)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(x.numpy(), sigmoid.numpy(), 'b-', linewidth=2)\n",
    "ax1.set_title(r'Sigmoid: $\\sigma(x) = 1/(1+e^{-x})$', fontsize=13)\n",
    "ax1.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlabel('x')\n",
    "\n",
    "ax2.plot(x.numpy(), sigmoid_grad.numpy(), 'r-', linewidth=2)\n",
    "ax2.axhline(y=0.25, color='gray', linestyle='--', alpha=0.5, label='max = 1/4')\n",
    "ax2.fill_between(x.numpy(), sigmoid_grad.numpy(), alpha=0.2, color='red')\n",
    "ax2.set_title(r\"Sigmoid derivative: $\\sigma'(x) = \\sigma(x)(1-\\sigma(x))$\", fontsize=13)\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlabel('x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum value of $\\sigma'(x)$ is $\\frac{1}{4}$ (at $x=0$). This means that at each layer, the gradient is multiplied by a factor $\\leq \\frac{1}{4}$.\n",
    "\n",
    "For a network with $L$ layers using sigmoid activations:\n",
    "\n",
    "$$\n",
    "\\left\\|\\frac{\\partial \\mathcal{L}}{\\partial W_1}\\right\\| \\leq \\left(\\frac{1}{4}\\right)^{L-1} \\cdot \\left\\|\\frac{\\partial \\mathcal{L}}{\\partial W_L}\\right\\|\n",
    "$$\n",
    "\n",
    "With $L = 10$ layers: $(1/4)^9 \\approx 4 \\times 10^{-6}$ — the gradient has **vanished**.\n",
    "\n",
    "\n",
    "In practice, we can compare gradient norms across layers for networks of increasing depth, using **sigmoid** activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_norms(model, X, y):\n",
    "    \"\"\"Perform one forward+backward pass and return gradient norms per layer.\"\"\"\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    logits = model(X)\n",
    "    loss = nn.CrossEntropyLoss()(logits, y)\n",
    "    loss.backward()\n",
    "    grad_norms = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name and param.grad is not None:\n",
    "            grad_norms.append(param.grad.norm().item())\n",
    "    return grad_norms\n",
    "\n",
    "\n",
    "X_spiral, y_spiral = make_spiral_data(n=1000)\n",
    "\n",
    "# Sigmoid networks of increasing depth\n",
    "sigmoid_models = [\n",
    "    ('4 layers', lambda: nn.Sequential(\n",
    "        nn.Linear(2, 20), nn.Sigmoid(), nn.Linear(20, 20), nn.Sigmoid(),\n",
    "        nn.Linear(20, 20), nn.Sigmoid(), nn.Linear(20, 20), nn.Sigmoid(),\n",
    "        nn.Linear(20, 2))),\n",
    "    ('8 layers', lambda: nn.Sequential(\n",
    "        nn.Linear(2, 20), nn.Sigmoid(), nn.Linear(20, 20), nn.Sigmoid(),\n",
    "        nn.Linear(20, 20), nn.Sigmoid(), nn.Linear(20, 20), nn.Sigmoid(),\n",
    "        nn.Linear(20, 20), nn.Sigmoid(), nn.Linear(20, 20), nn.Sigmoid(),\n",
    "        nn.Linear(20, 20), nn.Sigmoid(), nn.Linear(20, 20), nn.Sigmoid(),\n",
    "        nn.Linear(20, 2))),\n",
    "    ('16 layers', lambda: nn.Sequential(*(\n",
    "        [nn.Linear(2, 20), nn.Sigmoid()] +\n",
    "        [l for _ in range(15) for l in [nn.Linear(20, 20), nn.Sigmoid()]] +\n",
    "        [nn.Linear(20, 2)]))),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4.5))\n",
    "\n",
    "for ax, (name, make_model) in zip(axes, sigmoid_models):\n",
    "    torch.manual_seed(42)\n",
    "    model = make_model()\n",
    "    grad_norms = get_gradient_norms(model, X_spiral, y_spiral)\n",
    "    ax.bar(range(len(grad_norms)), grad_norms[::-1], color='indianred', alpha=0.8)\n",
    "    ax.set_title(name, fontsize=13)\n",
    "    ax.set_xlabel('Layer (output → input)', fontsize=11)\n",
    "    ax.set_ylabel('Gradient norm', fontsize=11)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Vanishing gradients with Sigmoid activation (at initialization)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Activation Functions\n",
    "\n",
    "### ReLU\n",
    "\n",
    "The ReLU activation function $\\text{ReLU}(x) = \\max(0, x)$ has derivative:\n",
    "\n",
    "$$\n",
    "\\text{ReLU}'(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\ 0 & \\text{if } x < 0 \\end{cases}\n",
    "$$\n",
    "\n",
    "Unlike Sigmoid (whose derivative is always $\\leq 1/4$), ReLU **preserves the gradient magnitude** for active neurons ($\\text{ReLU}'(x) = 1$). Given **proper initialization**, this keeps gradient norms roughly stable across layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare gradient norms: Sigmoid vs ReLU across depths\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "configs = [\n",
    "    ('Sigmoid (Xavier init)', nn.Sigmoid, lambda w: nn.init.xavier_normal_(w)),\n",
    "    ('ReLU (Kaiming init)',   nn.ReLU,    lambda w: nn.init.kaiming_normal_(w, nonlinearity='relu')),\n",
    "]\n",
    "\n",
    "depths = [4, 8, 16, 32]\n",
    "\n",
    "for ax, (act_name, Act, init_weight) in zip(axes, configs):\n",
    "    for d in depths:\n",
    "        torch.manual_seed(42)\n",
    "        model = nn.Sequential(*(\n",
    "            [nn.Linear(2, 20), Act()] +\n",
    "            [l for _ in range(d - 1) for l in [nn.Linear(20, 20), Act()]] +\n",
    "            [nn.Linear(20, 2)]))\n",
    "        for m in model:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init_weight(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "        grad_norms = get_gradient_norms(model, X_spiral, y_spiral)\n",
    "        ax.plot(range(len(grad_norms)), grad_norms[::-1], 'o-', label=f'{d} layers',\n",
    "                markersize=3, alpha=0.8)\n",
    "    ax.set_title(act_name, fontsize=14)\n",
    "    ax.set_xlabel('Layer (output → input)')\n",
    "    ax.set_ylabel('Gradient norm')\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Gradient norms at initialization (log scale)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU solved the vanishing gradient problem but introduced the **dead neuron** issue: if a neuron's input is always negative, it never activates and its weights never update.\n",
    "\n",
    "### Variations\n",
    "\n",
    "Several variants have been proposed:\n",
    "\n",
    "\n",
    "- **LeakyReLU**: $\\max(\\alpha x, x)$\n",
    "- **PReLU**: $\\max(\\alpha x, x)$\n",
    "- **ELU**: $x$ if $x>0$, $\\alpha(e^x-1)$ otherwise\n",
    "- **GELU**: $x \\cdot \\Phi(x)$\n",
    "- **SiLU/Swish**: $x \\cdot \\sigma(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize activation functions\n",
    "x = torch.linspace(-4, 4, 300)\n",
    "\n",
    "activations = {\n",
    "    'ReLU': F.relu(x),\n",
    "    'LeakyReLU': F.leaky_relu(x, 0.1),\n",
    "    'ELU': F.elu(x),\n",
    "    'GELU': F.gelu(x),\n",
    "    'SiLU (Swish)': F.silu(x),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, len(activations), figsize=(4 * len(activations), 3.5))\n",
    "\n",
    "for ax, (name, y_act) in zip(axes, activations.items()):\n",
    "    ax.plot(x.numpy(), y_act.numpy(), linewidth=2)\n",
    "    ax.axhline(y=0, color='gray', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='gray', linewidth=0.5)\n",
    "    ax.set_title(name, fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(-2, 4)\n",
    "\n",
    "plt.suptitle('Common Activation Functions', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare these alternative on our spiral dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare activations on spiral data with a 8-layer network\n",
    "X_spiral, y_spiral = make_spiral_data(n=1000)\n",
    "\n",
    "activations = [\n",
    "    ('Sigmoid',   nn.Sigmoid),\n",
    "    ('ReLU',      nn.ReLU),\n",
    "    ('LeakyReLU', nn.LeakyReLU),\n",
    "    ('GELU',      nn.GELU),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 4))\n",
    "\n",
    "for ax, (name, Act) in zip(axes, activations):\n",
    "    torch.manual_seed(42)\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(2, 20), Act(), nn.Linear(20, 20), Act(),\n",
    "        nn.Linear(20, 20), Act(), nn.Linear(20, 20), Act(),\n",
    "        nn.Linear(20, 20), Act(), nn.Linear(20, 20), Act(),\n",
    "        nn.Linear(20, 20), Act(), nn.Linear(20, 20), Act(),\n",
    "        nn.Linear(20, 2),\n",
    "    )\n",
    "    for m in model:\n",
    "        if isinstance(m, nn.Linear):\n",
    "            if Act == nn.Sigmoid:\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "            else:\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(3000):\n",
    "        logits = model(X_spiral)\n",
    "        loss = criterion(logits, y_spiral)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        final_acc = (model(X_spiral).argmax(1) == y_spiral).float().mean().item()\n",
    "\n",
    "    print(f'{name}: accuracy={final_acc:.2%}')\n",
    "    plot_decision_boundary(model, X_spiral, y_spiral, ax=ax,\n",
    "                          title=f'{name} (acc={final_acc:.0%})')\n",
    "\n",
    "plt.suptitle('8-layer MLP with different activations', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Degradation Problem\n",
    "\n",
    "Unfortunately, deeper plain networks (with ReLU) can have **higher training error** than their shallower counterparts. This is not overfitting — the model fails to fit even the *training data*.\n",
    "\n",
    "This phenomenon is called the **degradation problem**:\n",
    "\n",
    "\n",
    "A deeper model should be at least as expressive as a shallower one (the extra layers could, in theory, just learn the identity). But in practice, optimization struggles to find this solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degradation problem: deeper plain ReLU networks perform worse\n",
    "X_spiral, y_spiral = make_spiral_data(n=1000)\n",
    "\n",
    "depths = [4, 8, 16, 32]  # number of hidden layers\n",
    "\n",
    "fig, axes = plt.subplots(1, len(depths), figsize=(5 * len(depths), 4.5))\n",
    "all_histories = []\n",
    "\n",
    "for ax, n_hidden in zip(axes, depths):\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Build a plain network with n_hidden hidden layers of width 20\n",
    "    layers = [nn.Linear(2, 20), nn.ReLU()]\n",
    "    for _ in range(n_hidden - 1):\n",
    "        layers += [nn.Linear(20, 20), nn.ReLU()]\n",
    "    layers.append(nn.Linear(20, 2))\n",
    "    model = nn.Sequential(*layers)\n",
    "\n",
    "    # Kaiming initialization\n",
    "    for m in model:\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    # Train with SGD (not Adam) — Adam's adaptive LR masks the degradation problem\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    acc_history = []\n",
    "    for epoch in range(5000):\n",
    "        logits = model(X_spiral)\n",
    "        loss = criterion(logits, y_spiral)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                acc = (model(X_spiral).argmax(1) == y_spiral).float().mean().item()\n",
    "                acc_history.append(acc)\n",
    "\n",
    "    final_acc = acc_history[-1]\n",
    "    all_histories.append((n_hidden, acc_history))\n",
    "    print(f'{n_hidden} hidden layers: accuracy={final_acc:.2%}, params={n_params}')\n",
    "\n",
    "    plot_decision_boundary(model, X_spiral, y_spiral, ax=ax,\n",
    "                          title=f'{n_hidden} hidden layers\\nacc={final_acc:.0%}, {n_params} params')\n",
    "\n",
    "plt.suptitle('Plain ReLU networks of increasing depth (SGD)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training accuracy curves (smoothed with moving average)\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.9, len(depths)))\n",
    "\n",
    "for (n_hidden, acc_history), color in zip(all_histories, colors):\n",
    "    acc = np.array(acc_history)\n",
    "    smoothed = acc\n",
    "    epochs = np.arange(len(acc)) * 10\n",
    "    ax.plot(epochs, smoothed, color=color, linewidth=2, label=f'{n_hidden} hidden layers')\n",
    "\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Training Accuracy', fontsize=12)\n",
    "ax.set_title('Degradation Problem: Deeper ≠ Better (plain ReLU networks)', fontsize=13)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0.4, 1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deeper networks have **more parameters** and are **strictly more expressive**, yet they achieve lower **training** accuracy. This is not overfitting — it is an optimization failure.\n",
    "\n",
    "The degradation problem is distinct from vanishing gradients. Even with ReLU and Kaiming initialization, very deep networks are hard to optimize because:\n",
    "\n",
    "1. **The loss landscape becomes increasingly chaotic** — deeper networks have more saddle points and poor local minima.\n",
    "2. **Learning the identity is hard** — if the extra layers need to approximate the identity function to match a shallower network's performance, they must learn $W \\approx I$, which is a non-trivial optimization target for randomly initialized weights.\n",
    "3. **Small per-layer errors accumulate** — each layer introduces a small approximation error, and these errors compound through the depth of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going deeper: Residual Connections (ResNet)\n",
    "\n",
    " Instead of learning a mapping $\\mathcal{H}(x)$ directly, each block learns a **residual**:\n",
    "\n",
    "$$\n",
    "\\mathcal{H}(x) = x + \\mathcal{F}(x)\n",
    "$$\n",
    "\n",
    "where $\\mathcal{F}(x) = W_2 \\, \\sigma(W_1 x + b_1) + b_2$ is the residual branch.\n",
    "\n",
    "- If the optimal transformation is close to the identity, the network only needs to learn $\\mathcal{F}(x) \\approx 0$, which is easy.\n",
    "- Gradients flow directly through the skip connection: $\\frac{\\partial \\mathcal{H}}{\\partial x} = I + \\frac{\\partial \\mathcal{F}}{\\partial x}$, preventing vanishing gradients even with dozens of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"A residual block: output = x + F(x).\"\"\"\n",
    "    def __init__(self, width):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(width, width)\n",
    "        self.fc2 = nn.Linear(width, width)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.fc2(out)\n",
    "        return F.relu(out + residual)  # skip connection\n",
    "\n",
    "\n",
    "class PlainBlock(nn.Module):\n",
    "    \"\"\"A plain block (no skip connection): output = F(x).\"\"\"\n",
    "    def __init__(self, width):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(width, width)\n",
    "        self.fc2 = nn.Linear(width, width)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.fc2(out)\n",
    "        return F.relu(out)\n",
    "\n",
    "\n",
    "def make_deep_net(block_class, n_blocks, width=20):\n",
    "    \"\"\"Build a deep network by stacking blocks.\"\"\"\n",
    "    layers = [nn.Linear(2, width), nn.ReLU()]\n",
    "    for _ in range(n_blocks):\n",
    "        layers.append(block_class(width))\n",
    "    layers.append(nn.Linear(width, 2))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare **plain networks** (no skip connections) versus **ResNets** as we increase depth. Each block has 2 linear layers, so a network with $n$ blocks has $2n + 2$ total layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_spiral, y_spiral = make_spiral_data(n=1000)\n",
    "\n",
    "block_counts = [2, 5, 10, 25]  # 6, 12, 22, 52 total layers\n",
    "\n",
    "fig, axes = plt.subplots(2, len(block_counts), figsize=(5 * len(block_counts), 10))\n",
    "resnet_histories = {'Plain': [], 'ResNet': []}\n",
    "\n",
    "for j, n_blocks in enumerate(block_counts):\n",
    "    total_layers = 2 * n_blocks + 2\n",
    "    for i, (name, BlockClass) in enumerate([('Plain', PlainBlock), ('ResNet', ResidualBlock)]):\n",
    "        torch.manual_seed(42)\n",
    "        model = make_deep_net(BlockClass, n_blocks)\n",
    "        n_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "        # Train with Adam\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=3000)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        acc_history = []\n",
    "        for epoch in range(3000):\n",
    "            logits = model(X_spiral)\n",
    "            loss = criterion(logits, y_spiral)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                with torch.no_grad():\n",
    "                    acc = (model(X_spiral).argmax(1) == y_spiral).float().mean().item()\n",
    "                    acc_history.append(acc)\n",
    "\n",
    "        final_acc = acc_history[-1]\n",
    "        resnet_histories[name].append((n_blocks, total_layers, n_params, acc_history))\n",
    "        print(f'{name} {total_layers}L: accuracy={final_acc:.2%}, params={n_params}')\n",
    "\n",
    "        plot_decision_boundary(model, X_spiral, y_spiral, ax=axes[i, j],\n",
    "            title=f'{name} — {total_layers} layers\\nacc={final_acc:.0%}, {n_params} params')\n",
    "\n",
    "axes[0, 0].set_ylabel('Plain (no skip)', fontsize=14)\n",
    "axes[1, 0].set_ylabel('ResNet (skip)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training curves tell the same story: plain networks struggle to converge as depth grows, while ResNets maintain stable training regardless of depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves: Plain vs ResNet (smoothed)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.9, len(block_counts)))\n",
    "\n",
    "for idx, (n_blocks, total_layers, n_params, acc_history) in enumerate(resnet_histories['Plain']):\n",
    "    axes[0].plot(np.arange(len(acc_history)) * 10, acc_history,\n",
    "                 color=colors[idx], linestyle='--', alpha=0.7, linewidth=2,\n",
    "                 label=f'Plain {total_layers}L')\n",
    "for idx, (n_blocks, total_layers, n_params, acc_history) in enumerate(resnet_histories['ResNet']):\n",
    "    axes[0].plot(np.arange(len(acc_history)) * 10, acc_history,\n",
    "                 color=colors[idx], linestyle='-', alpha=0.9, linewidth=2,\n",
    "                 label=f'ResNet {total_layers}L')\n",
    "\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Training Accuracy (dashed=Plain, solid=ResNet)')\n",
    "axes[0].legend(fontsize=8, ncol=2)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Final accuracy vs depth\n",
    "plain_accs = [h[-1] for _, _, _, h in resnet_histories['Plain']]\n",
    "resnet_accs = [h[-1] for _, _, _, h in resnet_histories['ResNet']]\n",
    "total_layers_list = [2 * n + 2 for n in block_counts]\n",
    "\n",
    "axes[1].plot(total_layers_list, plain_accs, 'o--', color='indianred', linewidth=2,\n",
    "             markersize=8, label='Plain')\n",
    "axes[1].plot(total_layers_list, resnet_accs, 's-', color='seagreen', linewidth=2,\n",
    "             markersize=8, label='ResNet')\n",
    "axes[1].set_xlabel('Total layers')\n",
    "axes[1].set_ylabel('Final accuracy')\n",
    "axes[1].set_title('Final Accuracy vs Depth')\n",
    "axes[1].legend(fontsize=12)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim(0.4, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
