{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going Deep: Why Depth Matters\n",
    "\n",
    "_Adapted from [Dataflowr Module 14a](https://dataflowr.github.io/website/modules/14a-depth/) by Andrei Bursuc._\n",
    "\n",
    "**Why should we stack more layers?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Use GPU if available\n",
    "device = (\n",
    "    \"cuda:0\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "Let's start with 2D datasets of increasing complexity for a binary classification task (red/blue points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_linear_data(n=500):\n",
    "    \"\"\"Two Gaussian clusters, linearly separable.\"\"\"\n",
    "    X0 = torch.randn(n // 2, 2) * 0.6 + torch.tensor([-1.0, -1.0])\n",
    "    X1 = torch.randn(n // 2, 2) * 0.6 + torch.tensor([1.0, 1.0])\n",
    "    X = torch.cat([X0, X1], dim=0)\n",
    "    y = torch.cat([torch.zeros(n // 2), torch.ones(n // 2)]).long()\n",
    "    return X, y\n",
    "\n",
    "def make_circle_data(n=500):\n",
    "    \"\"\"Inner circle (class 0) vs outer ring (class 1).\"\"\"\n",
    "    theta = torch.linspace(0, 2 * np.pi, n // 2)\n",
    "    r_inner = torch.randn(n // 2) * 0.1 + 0.5\n",
    "    r_outer = torch.randn(n // 2) * 0.1 + 1.5\n",
    "    X_inner = torch.stack([r_inner * torch.cos(theta), r_inner * torch.sin(theta)], dim=1)\n",
    "    X_outer = torch.stack([r_outer * torch.cos(theta), r_outer * torch.sin(theta)], dim=1)\n",
    "    X = torch.cat([X_inner, X_outer], dim=0)\n",
    "    y = torch.cat([torch.zeros(n // 2), torch.ones(n // 2)]).long()\n",
    "    return X, y\n",
    "\n",
    "def make_spiral_data(n=500):\n",
    "    \"\"\"Two interleaved spirals — a classic hard 2D problem.\"\"\"\n",
    "    t = torch.linspace(0, 4 * np.pi, n // 2)\n",
    "    r = t / (4 * np.pi) * 2\n",
    "    noise = torch.randn(n // 2) * 0.08\n",
    "    X0 = torch.stack([(r + noise) * torch.cos(t), (r + noise) * torch.sin(t)], dim=1)\n",
    "    X1 = torch.stack([(r + noise) * torch.cos(t + np.pi), (r + noise) * torch.sin(t + np.pi)], dim=1)\n",
    "    X = torch.cat([X0, X1], dim=0)\n",
    "    y = torch.cat([torch.zeros(n // 2), torch.ones(n // 2)]).long()\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all three datasets\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "datasets = [\n",
    "    ('Linear', make_linear_data),\n",
    "    ('Circle', make_circle_data),\n",
    "    ('Spiral', make_spiral_data)\n",
    "]\n",
    "\n",
    "for ax, (name, make_data) in zip(axes, datasets):\n",
    "    X, y = make_data()\n",
    "    ax.scatter(X[y == 0, 0], X[y == 0, 1], c='royalblue', alpha=0.5, s=10, label='Class 0')\n",
    "    ax.scatter(X[y == 1, 0], X[y == 1, 1], c='crimson', alpha=0.5, s=10, label='Class 1')\n",
    "    ax.set_title(name, fontsize=14)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the standard PyTorch training loop with Adam optimizer (which we studied in [Chapter 6](../6-optimization/optimization.ipynb)). We train on the full batch (since our datasets are small) and track the loss and accuracy over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X, y, epochs=1000, lr=0.01):\n",
    "    \"\"\"\n",
    "    Train a classification model using Adam and CrossEntropyLoss.\n",
    "    Returns the training history (loss and accuracy per epoch).\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    history = {'loss': [], 'acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits, y)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        with torch.no_grad():\n",
    "            preds = logits.argmax(dim=1)\n",
    "            acc = (preds == y).float().mean().item()\n",
    "            history['loss'].append(loss.item())\n",
    "            history['acc'].append(acc)\n",
    "    \n",
    "    print(f\"Final loss: {history['loss'][-1]:.4f}, accuracy: {history['acc'][-1]:.2%}\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, ax=None, title=None, resolution=200):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary of a 2D classifier.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    \n",
    "    # Create grid\n",
    "    margin = 0.5\n",
    "    x_min, x_max = X[:, 0].min() - margin, X[:, 0].max() + margin\n",
    "    y_min, y_max = X[:, 1].min() - margin, X[:, 1].max() + margin\n",
    "    xx, yy = torch.meshgrid(\n",
    "        torch.linspace(x_min, x_max, resolution),\n",
    "        torch.linspace(y_min, y_max, resolution),\n",
    "        indexing='xy'\n",
    "    )\n",
    "    grid = torch.stack([xx.flatten(), yy.flatten()], dim=1)\n",
    "    \n",
    "    # Predict on grid\n",
    "    with torch.no_grad():\n",
    "        logits = model(grid)\n",
    "        preds = logits.argmax(dim=1).reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    cmap_bg = ListedColormap(['#AACCFF', '#FFAAAA'])\n",
    "    ax.contourf(xx.numpy(), yy.numpy(), preds.numpy(), alpha=0.3, cmap=cmap_bg)\n",
    "    \n",
    "    # Plot data points\n",
    "    ax.scatter(X[y == 0, 0], X[y == 0, 1], c='royalblue', alpha=0.5, s=10, edgecolors='k', linewidths=0.3)\n",
    "    ax.scatter(X[y == 1, 0], X[y == 1, 1], c='crimson', alpha=0.5, s=10, edgecolors='k', linewidths=0.3)\n",
    "    \n",
    "    if title:\n",
    "        ax.set_title(title, fontsize=12)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.2)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hidden Layer: The Universal Approximator\n",
    "\n",
    "In 1989, George Cybenko proved a remarkable result:\n",
    "\n",
    "> **Universal Approximation Theorem** (Cybenko, 1989; Hornik et al., 1989): A feedforward network with a single hidden layer containing a finite number of neurons can approximate any continuous function on compact subsets of $\\mathbb{R}^n$ to arbitrary precision.\n",
    "\n",
    "More precisely, for any continuous function $g: [0,1]^d \\to \\mathbb{R}$ and any $\\epsilon > 0$, there exists a network:\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_{i=1}^{N} \\alpha_i \\, \\sigma(w_i^T x + b_i)\n",
    "$$\n",
    "\n",
    "such that $\\|f - g\\|_\\infty < \\epsilon$, where $\\sigma$ is any non-constant, bounded, monotonically-increasing continuous activation function.\n",
    "\n",
    "This theorem says nothing about:\n",
    "- How many neurons $N$ are needed (it may be astronomically large)\n",
    "- Whether gradient descent can find the right parameters\n",
    "- Whether the resulting network generalizes well\n",
    "\n",
    "Let's verify experimentally that a single hidden layer can handle our non-linear problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, (name, make_data) in zip(axes, datasets):\n",
    "    X, y = make_data()\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(2, 100), nn.ReLU(), nn.Linear(100, 2)\n",
    "    )  # 1 hidden layer with 100 neurons\n",
    "    history = train_model(model, X, y, epochs=2000, lr=0.01)\n",
    "    plot_decision_boundary(model, X, y, ax=ax, title=f'1 Hidden Layer (100) — {name}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Universal Approximation Theorem guarantees that a sufficiently wide single-layer network **exists** that can solve the spiral problem. But:\n",
    "1. The required width may be very large\n",
    "2. Optimization may fail to find the right parameters\n",
    "3. The model may overfit with so many parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going Deeper: Depth vs Width\n",
    "\n",
    "We can compare two strategies for allocating parameters:\n",
    "\n",
    "- **Wide and shallow**: a single hidden layer with many neurons\n",
    "- **Narrow and deep**: multiple hidden layers with fewer neurons each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare architectures with roughly similar parameter counts\n",
    "architectures = [\n",
    "    ('Wide-shallow  [2, 100, 2]', lambda: nn.Sequential(\n",
    "        nn.Linear(2, 100), nn.ReLU(), nn.Linear(100, 2))),\n",
    "    ('Medium        [2, 20, 20, 2]', lambda: nn.Sequential(\n",
    "        nn.Linear(2, 20), nn.ReLU(), nn.Linear(20, 20), nn.ReLU(), nn.Linear(20, 2))),\n",
    "    ('Deep-narrow   [2, 15, 15, 15, 2]', lambda: nn.Sequential(\n",
    "        nn.Linear(2, 15), nn.ReLU(), nn.Linear(15, 15), nn.ReLU(),\n",
    "        nn.Linear(15, 15), nn.ReLU(), nn.Linear(15, 2))),\n",
    "]\n",
    "\n",
    "print(f'{\"Architecture\":<45} {\"Layers\":>8} {\"Params\":>8}')\n",
    "print('-' * 65)\n",
    "for name, make_model in architectures:\n",
    "    model = make_model()\n",
    "    n_hidden = sum(1 for m in model if isinstance(m, nn.ReLU))\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f'{name:<45} {n_hidden:>8} {n_params:>8}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us train all these architectures on the spiral dataset and compare their decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_spiral, y_spiral = make_spiral_data(n=1000)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "histories = []\n",
    "\n",
    "for ax, (name, make_model) in zip(axes, architectures):\n",
    "    torch.manual_seed(42)\n",
    "    model = make_model()\n",
    "    n_hidden = sum(1 for m in model if isinstance(m, nn.ReLU))\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    history = train_model(model, X_spiral, y_spiral, epochs=3000, lr=0.005)\n",
    "    histories.append((n_hidden, n_params, history))\n",
    "    plot_decision_boundary(model, X_spiral, y_spiral, ax=ax,\n",
    "                          title=f'{n_hidden} hidden, {n_params} params')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the training curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for n_hidden, n_params, history in histories:\n",
    "    label = f'{n_hidden} hidden ({n_params} params)'\n",
    "    ax1.plot(history['loss'], label=label, alpha=0.8)\n",
    "    ax2.plot(history['acc'], label=label, alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Features: What Each Layer Learns\n",
    "\n",
    "Formally, a deep network computes a composition of functions:\n",
    "\n",
    "$$\n",
    "f(x) = f_L \\circ f_{L-1} \\circ \\cdots \\circ f_1(x)\n",
    "$$\n",
    "\n",
    "where each $f_\\ell(z) = \\sigma(W_\\ell z + b_\\ell)$ is a layer that:\n",
    "1. Applies a linear transformation (rotation, scaling, shearing)\n",
    "2. Applies a non-linear activation (ReLU folds the space)\n",
    "\n",
    "Each layer transforms the data, making it progressively easier for the subsequent layers to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPWithIntermediates(nn.Module):\n",
    "    \"\"\"An MLP that stores intermediate activations for visualization.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 8), nn.ReLU(),\n",
    "            nn.Linear(8, 8), nn.ReLU(),\n",
    "            nn.Linear(8, 8), nn.ReLU(),\n",
    "            nn.Linear(8, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.intermediates = [x.detach().clone()]\n",
    "        for layer in self.model:\n",
    "            x = layer(x)\n",
    "            if isinstance(layer, nn.ReLU):\n",
    "                self.intermediates.append(x.detach().clone())\n",
    "        self.intermediates.append(x.detach().clone())\n",
    "        return x\n",
    "\n",
    "    def get_intermediates(self, x):\n",
    "        \"\"\"Forward pass that returns all intermediate activations.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.forward(x)\n",
    "        return self.intermediates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a deep network with 8D hidden layers for visualization\n",
    "torch.manual_seed(0)\n",
    "X_spiral, y_spiral = make_spiral_data(n=1000)\n",
    "\n",
    "model_vis = MLPWithIntermediates()\n",
    "\n",
    "# Train\n",
    "optimizer = torch.optim.Adam(model_vis.parameters(), lr=0.005)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(5000):\n",
    "    logits = model_vis(X_spiral)\n",
    "    loss = criterion(logits, y_spiral)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = model_vis(X_spiral).argmax(dim=1)\n",
    "    acc = (preds == y_spiral).float().mean()\n",
    "    print(f\"Accuracy: {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize intermediate representations (PCA projection to 2D)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "intermediates = model_vis.get_intermediates(X_spiral)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(intermediates), figsize=(4 * len(intermediates), 4))\n",
    "layer_names = ['Input'] + [f'After Layer {i+1}' for i in range(len(intermediates) - 1)]\n",
    "\n",
    "for i, (ax, Z, name) in enumerate(zip(axes, intermediates, layer_names)):\n",
    "    Z_np = Z.numpy()\n",
    "    if Z_np.shape[1] > 2:\n",
    "        Z_2d = PCA(n_components=2).fit_transform(Z_np)\n",
    "    else:\n",
    "        Z_2d = Z_np\n",
    "    ax.scatter(Z_2d[y_spiral == 0, 0], Z_2d[y_spiral == 0, 1],\n",
    "              c='royalblue', alpha=0.5, s=10)\n",
    "    ax.scatter(Z_2d[y_spiral == 1, 0], Z_2d[y_spiral == 1, 1],\n",
    "              c='crimson', alpha=0.5, s=10)\n",
    "    subtitle = f'{name}\\n(PCA)' if Z_np.shape[1] > 2 else name\n",
    "    ax.set_title(subtitle, fontsize=12)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('How each layer transforms the spiral data', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note_: The hidden representations are 8-dimensional; we use PCA to project them down to 2D for visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth Efficiency: More with Less\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Each ReLU layer can \"fold\" the space, and these folds compose multiplicatively.\n",
    "\n",
    "- **1 fold** (1 layer): creates 2 regions\n",
    "- **2 folds** (2 layers): creates 4 regions\n",
    "- **$k$ folds** ($k$ layers): creates $2^k$ regions\n",
    "\n",
    "A deep network with $k$ layers can carve the input space into $O(2^k)$ linear regions, while a shallow network with $N$ neurons can create at most $O(N)$ regions.\n",
    "\n",
    "To illustrate this, let's consider a checkerboard dataset where the number of regions grows, and compare how depth and width handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_checkerboard_data(n=2000, freq=2):\n",
    "    \"\"\"\n",
    "    Checkerboard pattern: class is determined by\n",
    "    (floor(freq*x) + floor(freq*y)) % 2.\n",
    "    Higher freq = more regions = harder problem.\n",
    "    \"\"\"\n",
    "    X = torch.rand(n, 2) * 2 - 1  # Uniform in [-1, 1]^2\n",
    "    y = ((torch.floor((X[:, 0] + 1) * freq) + torch.floor((X[:, 1] + 1) * freq)) % 2).long()\n",
    "    return X, y\n",
    "\n",
    "# Visualize checkerboards of increasing complexity\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for ax, freq in zip(axes, [1, 2, 3]):\n",
    "    X, y = make_checkerboard_data(freq=freq)\n",
    "    ax.scatter(X[y == 0, 0], X[y == 0, 1], c='royalblue', alpha=0.3, s=5)\n",
    "    ax.scatter(X[y == 1, 0], X[y == 1, 1], c='crimson', alpha=0.3, s=5)\n",
    "    n_regions = (2 * freq) ** 2\n",
    "    ax.set_title(f'Checkerboard freq={freq} ({n_regions} regions)', fontsize=12)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us compare a wide-shallow model with a deep-narrow model on checkerboards of increasing complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depth efficiency experiment\n",
    "freqs = [1, 2, 3]\n",
    "configs = {\n",
    "    'Shallow (1 hidden)': lambda: nn.Sequential(\n",
    "        nn.Linear(2, 100), nn.ReLU(), nn.Linear(100, 2)),\n",
    "    'Deep (4 hidden)': lambda: nn.Sequential(\n",
    "        nn.Linear(2, 20), nn.ReLU(), nn.Linear(20, 20), nn.ReLU(),\n",
    "        nn.Linear(20, 20), nn.ReLU(), nn.Linear(20, 20), nn.ReLU(),\n",
    "        nn.Linear(20, 2)),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(len(configs), len(freqs), figsize=(5 * len(freqs), 5 * len(configs)))\n",
    "\n",
    "for i, (config_name, make_model_fn) in enumerate(configs.items()):\n",
    "    for j, freq in enumerate(freqs):\n",
    "        torch.manual_seed(42)\n",
    "        X, y = make_checkerboard_data(n=2000, freq=freq)\n",
    "        model = make_model_fn()\n",
    "        n_params = sum(p.numel() for p in model.parameters())\n",
    "        history = train_model(model, X, y, epochs=2000, lr=0.005)\n",
    "        ax = axes[i, j]\n",
    "        plot_decision_boundary(model, X, y, ax=ax,\n",
    "            title=f'{config_name}\\nfreq={freq}, acc={history[\"acc\"][-1]:.0%}, params={n_params}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
